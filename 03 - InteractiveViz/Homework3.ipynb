{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 3  - INTERACTIVE VIZ\n",
    "\n",
    "** Build a Choropleth map which shows intuitively (i.e., use colors wisely) how much grant money goes to each Swiss canton. To do so, you will need to use the provided TopoJSON file, combined with the Choropleth map example you can find in the Folium README file.**\n",
    "\n",
    "**BONUS: using the map you have just built, and the geographical information contained in it, could you give a rough estimate of the difference in research funding between the areas divided by the Röstigraben?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the canton to which each grant was assigned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usual imports first.\n",
    "- A INSTALLER AVEC pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.geocoders import GeoNames\n",
    "from urllib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the grants and select a few useful attributes :\n",
    "- The university and the institution will try to help us locate the canton to which each grant was assigned.\n",
    "- The approved amount to know how much each canton received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grants = pd.read_csv('P3_GrantExport.csv',sep=';')\n",
    "df_grants = df_grants[[6,7,11,12,13]]\n",
    "df_grants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extracting the canton from the university field\n",
    "There are some issues in the data, that need to be formatted. First of all, some data need to be replaced with NaN, as they do not convey any information regarding the location of the grant that was assigned. We considered the following :\n",
    "- 'Nicht zuteilbar - NA' -> Unavailable data\n",
    "- 'NPO (Biblioth., Museen, Verwalt.) - NPO' -> No informations\n",
    "- 'Weitere Institute - FINST' -> Other institutes\n",
    "- 'Firmen/Privatwirtschaft - FP' -> Private institutions\n",
    "\n",
    "There is also a second part of data that simply have no university field. The location of where those grants were assigned then shall be determined in the second part of the extraction, where we turn our sights to the institution field of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_grants = df_grants.replace('Nicht zuteilbar - NA',np.nan)\n",
    "df_grants = df_grants.replace('NPO (Biblioth., Museen, Verwalt.) - NPO',np.nan)\n",
    "df_grants = df_grants.replace('Weitere Institute - FINST',np.nan)\n",
    "df_grants = df_grants.replace('Firmen/Privatwirtschaft - FP',np.nan)\n",
    "df_grants = df_grants[~(df_grants.University.isnull() & df_grants.Institution.isnull())]\n",
    "data_uni = df_grants['University'].unique()[1:]\n",
    "\n",
    "data_uni[1:10]\n",
    "df_grants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do a few less queries on the web to determine the location of our universities, we define two dictionnaries that we will extensively use to help us locate a university. The first one, `CANTON_DICT` associates each canton to its tag, that we will use on the map later on, but we can use it in the following way : if the key of this dict is present in the string describing the university, then we match it to the canton. It is very practical, as many universities are located in the capital of the canton, which often has the same name as the canton itself.\n",
    "\n",
    "The second dict, `CAPITAL_DICT` contains pairs associating the capital of a canton to a canton, for those who do not have exactly the same name. We will use it the same way as `CANTON_DICT` : if the name of the capital appears in the string of the university, we match it to the canton it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CANTON_DICT = {'Zürich':'ZH','Bern':'BE','Luzern':'LU','Uri':'UR','Schwyz':'SZ','Obwalden':'OW','Nidwalden':'NW',\n",
    "               'Glarus':'GL','Zug':'ZG','Fribourg':'FR','Solothurn':'SO','Basel-Stadt':'BS','Basel-Landschaft':'BL',\n",
    "               'Schaffhausen':'SH','Appenzell Ausserrhoden':'AR','Appenzell Innerrhoden':'AI','Sankt Gallen':'SG',\n",
    "               'Graubünden':'GR','Aargau':'AG','Thurgau':'TG','Ticino':'TI', 'Vaud':'VD','Valais':'VS','Neuchâtel':'NE',\n",
    "               'Genève':'GE','Jura':'JU','OUT': 'OUT'}\n",
    "CAPITAL_DICT = {'Basel':'Basel-Stadt' ,'Lausanne':'Vaud', 'Sion':'Valais','Altdorf':'Uri','Sarnen':'Obwalden','Stans':'Nidwalden',\n",
    "                'Liestal':'Basel-Landschaft','Herisau':'Appenzell Ausserrhoden',\n",
    "                'Chur':'Graubünden','Aarau':'Aargau','Frauenfeld':'Thurgau','Bellinzona':'Ticino','Delémont':'Jura'}\n",
    "#'Appenzell':'Appenzell Innerrhoden' excluded because the name of the city is partially in the name of two cantons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now start with the canton extraction from the uni field. We will use two methods for that. \n",
    "- `extract_canton` will also be reused later on, tries to find, given a data_string containing a university/institution, the canton if belongs to. It does that in three steps :\n",
    "    1. Tries to see if a canton appears in the data_string\n",
    "    2. Tries to see if a capital which does not have the same name as the canton appears in the data_string\n",
    "    3. Queries the Nominatim web service to try to locate the string. Here, if the response if `None`, we return `None`, and if the answer is not in Switzerland (its last field should contain `Svizra` in this case), then we also return `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_canton(data_string):\n",
    "    \"\"\"\n",
    "        Fetchs from the web the location associated to a string. We use Nominatim at the moment. \n",
    "        The format of the output string (if it is not None) has the canton in the 6th output before the end.\n",
    "        The canton is written in all the languages spoken in it, and we hence split and keep the name that comes first.\n",
    "        @param data_string : the data from which we want to determine the canton\n",
    "        @return canton : the name of the canton associated to the input data_string\n",
    "    \"\"\"\n",
    "    for canton in CANTON_DICT:\n",
    "        if(canton in str(data_string)):\n",
    "            return canton\n",
    "    for city,canton in CAPITAL_DICT.items():\n",
    "        if(city in str(data_string)):\n",
    "            return canton\n",
    "    geolocator = Nominatim()#,username='test_056')\n",
    "    location = geolocator.geocode(data_string)\n",
    "    split_loc = str(location).split(', ')\n",
    "    if (split_loc is not None) and (split_loc[-1] =='Svizra'):\n",
    "        canton = str(location).split(', ')\n",
    "        canton = canton[len(canton)-6]\n",
    "        return canton.split(' - ')[0]   \n",
    "    elif (split_loc != ['None']):\n",
    "        print(data_string)\n",
    "        print(split_loc)\n",
    "        return 'OUT'\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The second method, `extract_canton_from_uni`, will simply iterate on all the different universites of our DataFrame and call extract_canton. To get a higher chance of succeeding, we split the string at the `-` that separates the name and the acronym, and if the research was not successful with the full name, we pass the acronym to extract_canton as well. The full processing is done only once, and we store the result as a json file to load it again later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_canton_from_uni(data_uni):\n",
    "    \"\"\"\n",
    "        Extracts to which canton belongs a university and stored it into a json file (folder data).\n",
    "        @param data_uni :           an array of strings describing universities or other institutions.\n",
    "        @return university_dict :   a dict which associate to each university a canton if it was found\n",
    "                                    or a None if nothing was found.\n",
    "    \"\"\"\n",
    "    file = 'data/uni_canton_dict4.json'\n",
    "    if(os.path.isfile(file)):\n",
    "        print('Loading the data from json file')\n",
    "        with open(file) as f:\n",
    "            university_dict = json.load(f)\n",
    "    else:\n",
    "        print('Fetching the locations from the web')\n",
    "        university_dict = dict()\n",
    "        for uni in data_uni:\n",
    "            # Splits the university string at the '-', which usually corresponds to the separation between the full name\n",
    "            # and the acronym of the institution.\n",
    "            uni_name_split = uni.split(' - ')\n",
    "            extract_value = extract_canton(uni_name_split[0])\n",
    "            if (extract_value is 'OUT') and (len(uni_name_split) > 1):\n",
    "                # If the canton was not found in the name, then we try to extract it from the acronym string if it exists\n",
    "                extract_value = extract_canton(uni_name_split[1])\n",
    "            university_dict[uni] = extract_value\n",
    "        with open(file, 'w') as f:\n",
    "            json.dump(university_dict, f)\n",
    "    return university_dict\n",
    "university_canton = extract_canton_from_uni(data_uni)\n",
    "#university_canton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "university_canton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see the number of universities that were located and those who weren't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len([place for place, value in university_canton.items() if not pd.isnull(value)]))\n",
    "print(len([place for place, value in university_canton.items() if pd.isnull(value)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, this way only half of the universities were matches. Note that we did several tests and found that GeoNames leads a way poorer result than Nominatim (52 vs 37 not found), and prefiltering (trying to find the uni in the `CANTON_DICT` and `CAPITAL_DICT` dict) yields 37 not found against 40, but avoid queries on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now add a column to our grant DataFrame to find how many of the projects have been identified from their university name and fill this column with the canton tags from each canton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grants['Canton'] = np.nan\n",
    "match_canton_uni = lambda uni: CANTON_DICT[university_canton[uni]] if (not pd.isnull(university_canton[uni])) else university_canton[uni]\n",
    "df_grants.Canton = df_grants.University.loc[[uni in university_canton for uni  in df_grants.University]].apply(match_canton_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grants.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(df_grants[df_grants.Canton.isnull()]) / len(df_grants.Canton))\n",
    "print(len(df_grants[df_grants.University.isnull() & df_grants.Institution.isnull()]) / len(df_grants.Canton))\n",
    "df_grants[df_grants.Canton.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grants = df_grants[~(df_grants.University.isnull() & df_grants.Canton.isnull())]\n",
    "\n",
    "sum(df_grants.Canton.isnull()/len(df_grants.Canton))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that only 30% of the cantons were found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extracting the canton from the institution field\n",
    "Let us now address the potential problems we will encounter by displaying all the grants that have no universities associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_grants.Institution.loc[df_grants.University.isnull()].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that those institutions are mostly foreign universities or reasearch centers, that do not provide useful informations for our exercise. However, there are some swiss universities in the middle of it, so we will need to iterate through it and sort the institutions that will be kept or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_institution = df_grants['Institution'].loc[df_grants['Canton'].isnull()]\n",
    "df_institution = df_institution.unique()#head()\n",
    "print(df_institution)\n",
    "print(df_institution.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `function extract_canton_from_institution` will act the same way as `extract_canton_from_uni`, iterating over all the entries of institution we need (i.e. all the fields that were not found through the university canton extraction) and stores as well the result in a json.\n",
    "\n",
    "Note that as the number of institution is way larger than the number of universites, doing all our queries in one time will not be possible, due to the limitations from the website we're using. As we want to have the most consistent results as possible, we used the same website (Nominatim) for all the queries and simply waited when we had gone over the maximum number of queries we could make (around 2500 queries per person). We can easily recognise when we made too much queries, as our classification will return `NOT_FOUND` each time, meaning that the request timed out and that an exception was thrown, which we catches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def extract_canton_from_institution(data_institution, complete = True):\n",
    "    \"\"\"\n",
    "        Extracts to which canton belongs a institution and stored it into a json file (folder data). \n",
    "        As this data is very large, the queries to the website will most likely time out. We dump the the dictionnary \n",
    "        to a json file at each iteration and can restart from it with the parameter complete\n",
    "        @param data_institution :   an array of strings describing institutions.\n",
    "        @param complete         :   tells us if the process has been completed (everything saved to json file)\n",
    "        @return institution_dict :  a dict which associate to each university a canton if it was found\n",
    "                                    or a None if nothing was found.\n",
    "    \"\"\"\n",
    "    institution_dict = dict()\n",
    "    if(os.path.isfile('data/instit_canton_dict.json')):\n",
    "        print('Loading the data from json file')\n",
    "        with open('data/instit_canton_dict.json') as f:\n",
    "            institution_dict = json.load(f)\n",
    "    # We keep on sampling if the boolean complete is False, meaning that everything was not sampled yet.\n",
    "    if not(os.path.isfile('data/instit_canton_dict.json') and complete):\n",
    "        print('Fetching the locations from the web')\n",
    "        for index,instit in enumerate(data_institution):\n",
    "            try:\n",
    "                value = extract_canton(instit)\n",
    "            except :\n",
    "                #We catch the exception linked to the fact that the request timed out, and print 'NOT_FOUND'\n",
    "                #It is useful to realise that the we passed the limit number of requests we can ask to the server\n",
    "                value = np.nan\n",
    "                institution_dict[instit] = np.nan\n",
    "                print(index,'- NOT FOUND <===== ',instit,)\n",
    "            institution_dict[instit] = value\n",
    "            if value is not np.nan:\n",
    "                print(index,'-',value,'<=====',instit )\n",
    "        #We save in the json at the end of each iteration\n",
    "        with open('data/instit_canton_dict.json', 'w') as f:\n",
    "            json.dump(institution_dict, f)\n",
    "            excluded_bool = False\n",
    "    return institution_dict\n",
    "institution_canton = extract_canton_from_institution(df_institution[9000:],complete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now make sure that we treated every institution, by just seeing the length of the dictionnary we have. It is the same length as `df_institutions` as it only has unique entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Gets the lengths of the dictionnary stored so we can know where to sample from on the next run.\n",
    "with open('data/instit_canton_dict.json') as f:\n",
    "            institution_dict = json.load(f)\n",
    "print(institution_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_instit = df_grants.copy()\n",
    "df_instit['Canton'] = np.nan\n",
    "match_canton_insti = lambda uni: CANTON_DICT[institution_canton[uni]] if (not pd.isnull(institution_canton[uni])) else institution_canton[uni]\n",
    "df_instit.Canton = df_instit.Institution.loc[[uni in institution_canton for uni  in df_instit.Institution]].apply(match_canton_insti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_grants.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_instit.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index in df_grants.index :\n",
    " \n",
    " if df_grants.Canton.loc[index] != df_grants.Canton.loc[index] :\n",
    "    df_grants.Canton.loc[index] = df_instit.Canton.loc[index]\n",
    "\n",
    "df_grants.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sum(df_grants.Canton.isnull()/len(df_grants.Canton))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print (df_grants[[df_grants['Approved Amount']=='data not included in P3']])\n",
    "new = df_grants[df_grants['Approved Amount']!='data not included in P3']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Drawing the Switzerland map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "final = pd.DataFrame(columns=['prix'])\n",
    "final.indexes=['Canton']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for canton in CANTON_DICT :\n",
    " x = new[df_grants['Canton']==CANTON_DICT[canton]]['Approved Amount'].values\n",
    " x = x.astype(np.float)\n",
    " final.loc[CANTON_DICT[canton]]=[np.sum(x)/1e6]\n",
    "\n",
    "final.head()\n",
    "final = final.reset_index()\n",
    "final.head()\n",
    "final.to_csv('grants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "state_geo = r'data/ch-cantons.topojson.json'\n",
    "state_grants = r'data/grants.csv'\n",
    "\n",
    "state_data = pd.read_csv(state_grants)\n",
    "\n",
    "print(state_data['prix'])\n",
    "#Let Folium determine the scale\n",
    "#ice_map = folium.Map(location=[-59.1759, -11.6016],\n",
    "#                   tiles='Mapbox Bright', zoom_start=2)\n",
    "#ice_map.choropleth(geo_path=state_geo, topojson='objects.cantons')\n",
    "color_map = folium.Map(location=[48, -102], zoom_start=3)\n",
    "color_map.choropleth(geo_path=state_geo,topojson='objects.cantons', data=state_data,\n",
    "             columns=['index', 'prix'],\n",
    "             threshold_scale=[1,500,1000,2000,3000,4000] ,\n",
    "             key_on='id',\n",
    "             fill_color='YlGn', fill_opacity=0.7, line_opacity=0.2,\n",
    "             legend_name='canton grants (MCHF)')\n",
    "color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CANTON_ROMAN = ['GE', 'VD', 'FR', 'JU', 'VS', 'NE']\n",
    "CANTON_ALLEMAND = ['ZH','BE','LU','UR','SZ','OW','NW','GL','ZG','SO','BS','BL','SH','AR','AI','SG','GR','AG','TG']\n",
    "\n",
    "roman_fund = df_grants[[x in CANTON_ROMAN for x in df_grants.Canton.values]]\n",
    "roman_fund = roman_fund[roman_fund['Approved Amount']!='data not included in P3']\n",
    "roman_fund_amount = np.sum(roman_fund['Approved Amount'].values.astype(np.float))\n",
    "print(roman_fund_amount / 1e6)\n",
    "\n",
    "allemand_fund = df_grants[[x in CANTON_ALLEMAND for x in df_grants.Canton.values]]\n",
    "allemand_fund = allemand_fund[allemand_fund['Approved Amount']!='data not included in P3']\n",
    "allemand_fund_amount = np.sum(allemand_fund['Approved Amount'].values.astype(np.float))\n",
    "print(allemand_fund_amount / 1e6)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
