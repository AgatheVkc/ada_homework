{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Supervised Learning\n",
    ">Train a `sklearn.ensemble.RandomForestClassifier` that given a soccer player description outputs his skin color. Show how different parameters passed to the Classifier affect the overfitting issue. Perform cross-validation to mitigate the overfitting of your model. Once you assessed your model, inspect the feature\\_importances\\_ attribute and discuss the obtained results. With different assumptions on the data (e.g., dropping certain features even before feeding them to the classifier), can you obtain a substantially different feature\\_importances\\_ attribute?\n",
    "\n",
    "> *BONUS:* plot the learning curves against at least 2 different sets of parameters passed to your Random Forest. To obtain smooth curves, partition your data in at least 20 folds. Can you find a set of parameters that leads to high bias, and one which does not?\n",
    "\n",
    "From the preprocessing step, we generated 6 datasets, that we will use with our machine learning algorithm. We will later on compare the results we obtain. As a reminder, the 6 datasets are as follows\n",
    "1.  3 datasets with NaNs and 3 without NaNs\n",
    "\n",
    "2.  1. players.csv -> Simply the players, no added cross-features\n",
    "    2. players_with features.csv -> Added cross-features.\n",
    "    3. players_with features_only_num_values.csv -> We do not consider the textual fields.\n",
    "    \n",
    "    \n",
    "## 1.0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, cross_val_predict, learning_curve\n",
    "import sklearn.metrics\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# There's a lot of columns in the DF. \n",
    "# Therefore, we add this option so that we can see more columns\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Preparing the features\n",
    "Given a file, loads the features the way we need them to pass them to the `RandomForestClassifier` method. We first create a field named *X* which contains all the features (entries of the loaded DataFrame minus the *skin_colour* column of it). The *Y* will will be the output labels, containing the informations of the *skin_colour* columns DataFrame, but the values will be rescaled into integers. Finally, the *columns* fields contains the name of the features of *X*.\n",
    "\n",
    "\n",
    "We also prepare the output labels : first of all, we do a binary classification, setting the limit at a *threshold* which is by default fixed at *0.5*, in order to have a simple case to treat. We return as well the columns vector, which corresponds to the features we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_features_binary(file_,thresh = 0.5):\n",
    "    \"\"\"\n",
    "        Loads the data contained in the file_ and formats it in the way we need for it to be fed into a sklearn classifier. \n",
    "        @param file_ : the file which contains the data we want to learn on.\n",
    "        @param thresh : the threshold at which we set the binary classification (x >= thresh -> y = 1, x < thresh -> y =0)\n",
    "        @return X : the features of the model (size n_samples x n_features)\n",
    "        @return Y : the labels of our set (size n_samples)\n",
    "        @return columns : the name of the features (list of string of size n_features)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_)\n",
    "\n",
    "    # We drop the skin colour as it is the solution, it goes into the output array.\n",
    "    X = df.drop('skin_colour',axis=1)\n",
    "    \n",
    "    columns = X.columns.values\n",
    "    \n",
    "    Y = np.array(df['skin_colour'])\n",
    "    \n",
    "    # Binary classification performed here.\n",
    "    Y = (Y>=thresh).astype(int)\n",
    "    \n",
    "    return X, Y, columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second preparation splits the data into a given number of classes, in order to see how we perform which less aggregated data. It outputs a label from *0* to *split* for the *Y* attribute. \n",
    "\n",
    "An example is worth a thousand words. Our data *Y* are takes values in one of those : \n",
    "$y_i \\in [0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1]$\n",
    "\n",
    "Let us suppose our Y has the form  $Y = [0, 0.25, 0.5, 0.5, 0.75, 1, 0.625]$\n",
    "Hence, aggregating our data in 3 classes will yield the the Y's to go into \n",
    "- The class labelled 0 if $y_i \\in [0, 0.125, 0.25] $\n",
    "- The class labelled 1 if $y_i \\in [0.375, 0.5, 0.625]$\n",
    "- The class labelled 2 if $y_i \\in [0.75, 0.875, 1]$\n",
    "\n",
    "Thus, our example will end up being of the form $Y_{agg} = [0, 0, 1, 1, 2, 2, 1]$. This is what the routine `prepare_features_multi` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_features_multi(file_,split):\n",
    "    \"\"\"\n",
    "        Loads the data contained in the file_ and formats it in the way we need for it to be fed into a sklearn classifier.\n",
    "        The form of the output is the following  : x in the interval [ split[i], split[i+1] [ -> output y = i\n",
    "        @param file_ : the file which contains the data we want to learn on.\n",
    "        @param split : the number of different output classes that we will have (2 to 9 classes only)\n",
    "        @return X : the features of the model (size n_samples x n_features)\n",
    "        @return Y : the labels of our set (size n_samples)\n",
    "        @return columns : the name of the features (list of string of size n_features)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_)\n",
    "    # We drop the skin colour as it is the solution, it goes into the output array.\n",
    "    X = df.drop('skin_colour',axis=1)\n",
    "    columns = X.columns.values\n",
    "    \n",
    "    \n",
    "    Y = np.array(df['skin_colour'])\n",
    "    #Array which contains the values at which we classify into a category or the next one\n",
    "    splits = np.linspace(0,1,split+1)\n",
    "    \n",
    "    #Array which starts at a basis negative (invalid value) and contains the labels of which of the \n",
    "    #class in which the data entry is.\n",
    "    Y_agg = -2*np.ones(len(Y))\n",
    "    \n",
    "    for i in range(0, split):\n",
    "        Y_agg[np.logical_and(Y >= splits[i], Y < splits[i+1])] = i\n",
    "    Y_agg[Y==1] = i\n",
    "    \n",
    "    return X, Y_agg,columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Classification of our data.\n",
    "The classifier we were asked to use is the Random Forest. In order to evaluate the performance of our results, we will use several tools, which will help us to understand better the results we obtain. \n",
    "\n",
    "1. The [cross_validation module of scikit-learn](http://scikit-learn.org/0.17/modules/generated/sklearn.cross_validation.cross_val_score.html) allows us to test the performance of our classification. The `cross_val_score` method returns the percentage of accuracy of our classification (average of the testing error of each iteration of the cross-validation. The `cv` field allows us to chose the number of folds of cross-validation we want to perform (e.g. cv=5 -> 5-fold cross-validation).\n",
    "2. The [F1 score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) takes into account the false positives and the false negatives in the process of outputting the score. Hence, a model with a high prediction accuracy can get very poor results in the F1-metric if for instance it outputs everything to an output which is dominant in the population (cf. the examples \"Everybody has cancer\" in the lecture 07 of the course).\n",
    "3. [The confusion matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) plots the detail of the classification, and allows us to visualise the false positives, false negatives. We can compute the F1-score from this matrix.\n",
    "4. The [feature importances](http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html) allows us to see which of the features are the most significant for the classification. \n",
    "\n",
    "\n",
    "If we turn now to the coding, the first function below `print_confusion_matrix` is a helper function that helps us visualising the confusion matrix in a more elegant way than the usual way of displaying a 2D numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix(conf_matrix):\n",
    "    \"\"\"\n",
    "        Prints in textual way the entries of the confusion matrix conf_matrix and allows us to visualise it in a \n",
    "        more elegant way than when displaying it simply with print. It makes the understanding of it more intuitive.\n",
    "    \"\"\"\n",
    "    print('\\n\\t\\t\\tDISPLAYING THE CONFUSION MATRIX\\n')\n",
    "    print('\\tPredicted : ',end='\\t' )\n",
    "    features = conf_matrix.shape[0]\n",
    "    for i in range(0,features):\n",
    "        print(i,end='\\t')\n",
    "    print('TOTAL')\n",
    "    print('\\n Reality : ',end='\\t')\n",
    "    for i in range(0,features):\n",
    "        print(i,'||',end='\\t')\n",
    "        for j in range(0,features):\n",
    "            print(conf_matrix[i,j],end='\\t')\n",
    "        print(sum(conf_matrix[i,:]),end='\\n\\t\\t')\n",
    "    print('\\nTOTALS : \\t  ||',end='\\t')\n",
    "    for j in range(0,features):\n",
    "        print(sum(conf_matrix[:,j]),end='\\t')\n",
    "    print(sum(sum(conf_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cross_validation` function does perform the *cv_param*-fold cross-validation, and outputs the cross-validation result, along with F1-score and the confusion matrix, in order for us to understand the shape of our results. We perform by default a 20-fold cross validation, as we want a result as stable as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation(X,Y,cv_param = 20, max_depth = None): \n",
    "    \"\"\"\n",
    "        Cross_validation takes as input a dataset X and the labels Y and performs the cv_param-fold cross validation.\n",
    "        It uses a random forest classifier in order to do so and plots the cross-validation score, the f1-score\n",
    "        as well as the confusion matrix.\n",
    "    \"\"\"\n",
    "    # 1. Creates the classifier that we will use \n",
    "    forest = RandomForestClassifier(max_depth = max_depth)\n",
    "    # 2. Predicts the output of the classification after cv_param-fold cross-validation\n",
    "    Y_predicted = cross_val_predict(forest, X, Y, cv=cv_param)\n",
    "    \n",
    "    # 3. Print the results : scores, \n",
    "    print('Cross Validation result :', cross_val_score(forest,X,Y,cv = cv_param).mean(),\n",
    "        '\\nF1 score result :',sklearn.metrics.f1_score(Y, Y_predicted,average='micro'))\n",
    "    \n",
    "    print_confusion_matrix(sklearn.metrics.confusion_matrix(Y,Y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot_feature_importances` function does rank each feature of *X* accordingly to the role it plays into the classification of the data we give it. This allows us to see whether a key subset of our features would turn out to be outstandingly better than the rest at determining the skin colour of a player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(X, Y, columns,plot_flag = 1, max_depth = None):\n",
    "    \"\"\"\n",
    "        plot_feature_importances displays the importance of each feature into determining the output of our problem. \n",
    "        @param plot_flag : a boolean which can be turned on or off for the plotting for the display of the results.\n",
    "    \"\"\"\n",
    "    forest = RandomForestClassifier(max_depth = max_depth)\n",
    "    forest = forest.fit(X,Y)\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    if(plot_flag):    \n",
    "        for f in range(X.shape[1]):\n",
    "            print(\"%d. feature %d - %s (%f)\" % (f + 1, indices[f],  columns[indices[f]], importances[indices[f]]))\n",
    "            \n",
    "        # Plot the feature importances of the forest\n",
    "        plt.figure()\n",
    "        plt.title(\"Feature importances\")\n",
    "        plt.bar(range(X.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
    "        plt.xticks(range(X.shape[1]), indices)\n",
    "        plt.xlim([-1, X.shape[1]])\n",
    "        plt.show()\n",
    "    return indices,importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3. Bonus part \n",
    "> *BONUS:* plot the learning curves against at least 2 different sets of parameters passed to your Random Forest. To obtain smooth curves, partition your data in at least 20 folds. Can you find a set of parameters that leads to high bias, and one which does not?\n",
    "\n",
    "It turns out that the learning curves are useful to be able to see whether our model is massively overfitting, and to help tune the best parameters on which to run our model. We will then plot the learning curves here and will pick the best model we have for the rest of the exercise. \n",
    "\n",
    "We will load different data and compare the learning curves on them. We consider 3 different number of classes : 2, 5 and 9 classes of skin colour, in which we classify the players. We load also two different datasets : the first where where did not add any features, and a second one with the featuers we added)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data without features, classified into 2-5-9 classes\n",
    "X_no_ft_binary,Y_no_ft_binary,columns_no_ft_binary = prepare_features_multi('Data/players_no_nan_only_num_values.csv',2)\n",
    "X_no_ft_5,Y_no_ft_5,columns_no_ft_5 = prepare_features_multi('Data/players_no_nan_only_num_values.csv',5)\n",
    "X_no_ft_9,Y_no_ft_9,columns_no_ft_9 = prepare_features_multi('Data/players_no_nan_only_num_values.csv',9)\n",
    "\n",
    "# Data with features added, classified into 2-5-9 classes.\n",
    "X_2, Y_2, columns_2 = prepare_features_multi('Data/players_with features_no_nan_only_num_values.csv',2)\n",
    "X_5, Y_5, columns_5 = prepare_features_multi('Data/players_with features_no_nan_only_num_values.csv',5)\n",
    "X_9 ,Y_9, columns_9 = prepare_features_multi('Data/players_with features_no_nan_only_num_values.csv',9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the extent of plotting the learning curves, we will use the [learning_curve function of scikit learn](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html). As asked in the exercise, we perform split our data in 20 folds and will also perform a 20-fold cross validation each time, as we did throughout the rest of the exercise. The function that plots the learning curves was *largely* taken from [this example](http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py) on the scikit learn website. \n",
    "\n",
    "The first result that we plot is the learning curves using the very basic *Random Forest Classifier*, without any argument passed. It means that, for instance, the maximal depth of the tree is not fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator,X,Y,title,cv=20):\n",
    "    \"\"\"\n",
    "        Custom plotting routine to plot the training and testing scores against the number of data considered.\n",
    "        This takes an estimator and a Dataset as inputs, along with their labels, and the k-folds of cross validation\n",
    "        and simply plots them with their standard deviation added as colour on the plot. \n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, Y, cv=cv, train_sizes=np.linspace(0.2,1,20))\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                    train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "            label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "            label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "estimator = RandomForestClassifier()    \n",
    "\n",
    "title = \"Learning Curves (Random Forest Classifier with 2 classes and no added features)\"\n",
    "plot_learning_curve(estimator,X_no_ft_binary,Y_no_ft_binary,title,20)   \n",
    "\n",
    "title = \"Learning Curves (Random Forest Classifier with 2 classes and added features)\"\n",
    "plot_learning_curve(estimator,X_2,Y_2,title,20)    \n",
    "    \n",
    "    \n",
    "title = \"Learning Curves (Random Forest Classifier with 9 classes and no added features)\"    \n",
    "plot_learning_curve(estimator,X_no_ft_9,Y_no_ft_9,title,20)\n",
    "\n",
    "title = \"Learning Curves (Random Forest Classifier with 9 classes and added features)\"\n",
    "plot_learning_curve(estimator,X_9,Y_9,title,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4 plots describes the classification into binary output and 9-classes output respectively, with and without features. They all have the very obvious same problem. The default *RandomForestClassifier* method clearly overfits our data, this is notably due to the fact of having an unfixed depth for the depth of the tree. This is why we will iterate over different depth and fix it to a value that yields a good result, and the fact of having a max depth will mitigate overfitting.\n",
    "\n",
    "We now compute the result of the cross validation given a certain depth, varying from 1 to 20. Do not look too much at the `plot_fig` function, which only provides visulisation of our data obtained while iterating on the max_depth of the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_score_2 = np.zeros(20)\n",
    "cv_score_9 = np.zeros(20)\n",
    "\n",
    "tr_score_2 = np.zeros(20)\n",
    "tr_score_9 = np.zeros(20)\n",
    "\n",
    "cv_param = 20\n",
    "for i in range(1,21):\n",
    "    forest = RandomForestClassifier(max_depth = i)\n",
    "    cv_score_2[i-1] = cross_val_score(forest,X_2,Y_2,cv = cv_param).mean()\n",
    "    cv_score_9[i-1] = cross_val_score(forest,X_9,Y_9,cv = cv_param).mean()\n",
    "    forest.fit(X_2,Y_2)\n",
    "    tr_score_2[i-1] = forest.score(X_2,Y_2)\n",
    "    forest.fit(X_9,Y_9)\n",
    "    tr_score_9[i-1] = forest.score(X_9,Y_9)\n",
    "\n",
    "def plot_fig(data_1,data_2,title_,xlabel_,ylabel_):\n",
    "    \"\"\"\n",
    "        Custom plotting routine to plot our data the way we want \n",
    "        (does not provide anything extremely useful)\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)    # The big subplot\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    ax2 = fig.add_subplot(212)\n",
    "\n",
    "    # Turn off axis lines and ticks of the big subplot\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['bottom'].set_color('none')\n",
    "    ax.spines['left'].set_color('none')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.tick_params(labelcolor='w', top='off', bottom='off', left='off', right='off')\n",
    "\n",
    "    ax1.plot(np.linspace(1,20,20), data_1, 'o-', color=\"r\",label=\"2 classes\")\n",
    "    ax2.plot(np.linspace(1,20,20), data_2, 'o-', color=\"g\",label=\"9 classes\")\n",
    "    ax1.legend(loc=\"best\")\n",
    "    ax2.legend(loc=\"best\")\n",
    "    ax.set_title(title_)\n",
    "    ax.set_xlabel(xlabel_)\n",
    "    ax.set_ylabel(ylabel_)\n",
    "    plt.show()\n",
    "\n",
    "plot_fig(cv_score_2,cv_score_9,\n",
    "         \"Cross validation score again the depth of the random forest\",\n",
    "         \"Max depth of the random forest\",\"Cross validation score\")    \n",
    "plot_fig(tr_score_2,tr_score_9,\"Training score again the depth of the random forest\",\n",
    "        \"Max depth of the random forest\",\"Testing score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for each of the cases, the accuracy of the precision does not change very much, even if the overfitting is very different. We also want to see which of the depth minimizes the overfitting (i.e. gets the lowest accuracy on the training set). The one we chose is the *max_depth = 7*, which we plot below for the data with added features. We took this entry as it is a good compromise between a good cross-validation score and a low testing score, which means no overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator = RandomForestClassifier(max_depth = 7)    \n",
    "\n",
    "title = \"Learning Curves (Random Forest Classifier with 2 classes and added features)\"\n",
    "plot_learning_curve(estimator,X_2,Y_2,title,20)    \n",
    "\n",
    "title = \"Learning Curves (Random Forest Classifier with 9 classes and added features)\"\n",
    "plot_learning_curve(estimator,X_9,Y_9,title,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results that we get in this case are way more reassuring than those we got without fixing the maximal depth of the tree, without decresing the accuracy of the cross-validation score. It means that for data that our algorithm did not train on before, it will perform better than its counterpart without fixed depth, as it tends less to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Comparison of the results for different models.\n",
    "Having found the depth at wich our tree does not overfit, we want to focus on understanding the results we get. We will compare again the raw data, taking only the numeric values of the basic features we were given and adding the country in which the player is currently playing as fields as well along with, in a second time, the features we added. We will iterate over different splits (i.e. binary classification, 5 classes, and 9 classes) and see the feature importances evolution with the number of classes we have for the classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Binary classification\n",
    "print('STARTING WITH BINARY CLASSIFICATION\\n')\n",
    "cross_validation(X_no_ft_binary, Y_no_ft_binary, cv_param=20, max_depth=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the raw features and the binary classification, one can immediately that there is a huge problem ! The first problem comes from the data themselves, splitting in two groups of people at a threshold of 0.5, we get two very inegal groups. One with output 1 (white) contains 1089 entries, while the second one (coloured) contains 330 entries only. There is a quite large imbalance here. We get roughly 75% of the population in the first class, and only 25% percent in the second one.\n",
    "\n",
    "\n",
    "The classification, which actually got an accuracy of 75% is quite wrong, as it nearly classifies all the entries as 0, which gives 75% percent accuracy, as 75% percent of the population is in his class. We see that the problem lies in the fact that for the class 1, nearly all its entries (292 people), are wrongly classified as belonging to the class 0, while only 40 are correctly classified ! Only 5% of the members of the class with label 1 are classified correctly, against 98% of the members of the class 0. We hence see that our model with the raw features is not a good classifier. \n",
    "\n",
    "Now, we will preform the 5/9-classes classification, and see how the prediction evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#5 classes classification\n",
    "print('\\n5 CLASSES CLASSIFICATION\\n')\n",
    "cross_validation(X_no_ft_5, Y_no_ft_5, cv_param=20, max_depth=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shows an accuracy of 46%, confirmed by the F1-metric score. The problem that shows here is exactly the same as before. The classes with a lot of entries (class with label 0 and 1 (i.e. skin_colour from 0 to 0.4) get almost all the entries : 1383 out of 1419 entries, while \"only\" containing in reality 1089 entries. We get 26% people more than there should be in these classes, with only actually 57% percent of the people correctly classified. And the problems shows that nearly all the people from the smaller classes (resp. 103,113,114 entries) are very much missclassified, with only in total 10% of people that we should have across those 3 classes, and only 4 people classified correctly across those 3 classes (which represents 1% of the entries) ! Another problem also arises here, it is the fact that a lot of people of the class 0 are wrongly classificated in the class 1, and a lot of people of the class 1 are wrongly classified into the class 0, yielding a very large error as well. The ther problem is also that none of the people from the class 2 and 3 get successfully classified in their respective classes.\n",
    "\n",
    "If we now turn to the 9 classes classification, we would expect to get the same behaviour as here, only with more \"extreme\" cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#9 classes classification\n",
    "print('\\n9 CLASSES CLASSIFICATION\\n')\n",
    "forest_no_ft = cross_validation(X_no_ft_9, Y_no_ft_9, cv_param=20, max_depth=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our results again are in the same fasion as before : the classes with a lot of entries (here the classes 0 and 2) get a lot of entries, and actually a lot more than they should (619 and 711 against 386 and 419 resp.). The error of classification is again huge and is due a lot to the fact that roughly half of the entries of the class 0 are classified into class 2 and inversely ! The classifier does not do at all any difference between both entries. There is also another case of interest in the fact that the 0 elements gets classified into class 5, while there are actually 28 elements in it ! It shows that having only the basic features is clearly not suffificent for our model to be able to classify the players in a reliable manner. Moreover, the classes 4,5,6,7,8 have 0 entries that are correctly classified into these respective classes, showing how poorly our classifier performs given the data we have.\n",
    "\n",
    "Let us now turn to the feature importances and try to see whether some feature stands out in every case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features_2 = plot_feature_importances(X_no_ft_binary,Y_no_ft_binary,columns_no_ft_binary, max_depth=7)\n",
    "features_5 = plot_feature_importances(X_no_ft_5, Y_no_ft_5, columns_no_ft_5, max_depth=7)\n",
    "features_9 = plot_feature_importances(X_no_ft_9, Y_no_ft_9,columns_no_ft_9, max_depth=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot to say from these feature importances. Namely, the trends are the same for the three models, with a different number of classes. This shows that even if our results are not good, they are produced from the model with a consistent approach, and are maybe due to the fact that it is actually impossible to reliably predict the colour of the skin of a player from the features we considered here. \n",
    "\n",
    "On the results we just saw, one can clearly see that none of the features appears to be particularly significant : a few of them lie just around 10% of importance, 2-3 are intermediate and 3-4 are not useful. The ranking is as follows :\n",
    "\n",
    "- **Most useful features :**  games, yellowCards, goals, defeats, ties , victories, weight, birthYear, height \n",
    "- **Intermediate :** yellowReds, redCards, (french)\n",
    "- **Not useful :** (french), spanish, german, english\n",
    "\n",
    "It is interesting to constast that those results are consistent throughout the different runs. The *french* feature, which means the player is playing in France, seems to be just slightly more efficient in the binary classification, for an unknown reason. If one looks at those features, it seems indeed hard to determine the skin colour of a player from those data, as none of those features seem efficient in doing the difference. We will now try to redo the same pipeline, but with the features we created, which are linked to the bias of the referee towards the skin colour of the player, and we'll see if we obtain better results.\n",
    "\n",
    "Let us again first prepare our features, then cross-validate them and see the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2 classes classification\n",
    "\n",
    "print('\\n\\nRESULTS WITH NO FEATURES ADDED\\n')\n",
    "cross_validation(X_no_ft_binary, Y_no_ft_binary, cv_param=20, max_depth=7)\n",
    "\n",
    "print('\\n2 CLASSES CLASSIFICATION\\n')\n",
    "cross_validation(X_2, Y_2, cv_param=20, max_depth=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We almost get the same result as before for the binary classification ! Just around 20 elements difference in the total, and 10-20 classification changing across the bord. It seems that the features we worked hard to implement do not bring an improvement to our results, at least for binary classification :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#5 classes classification\n",
    "print('\\n\\nRESULTS WITH NO FEATURES ADDED\\n')\n",
    "cross_validation(X_no_ft_5, Y_no_ft_5, cv_param=20, max_depth=7)\n",
    "\n",
    "print('\\n5 CLASSES CLASSIFICATION\\n')\n",
    "cross_validation(X_5, Y_5, cv_param=20, max_depth=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering now the 5-fold classification,  we get to the same conclusion as before. The addition of our features does not seem to change a lot the results, just increasing their accuracy from 43.4% to 44.4%. This seems to suggest than even if we add the bias of the referee into the balance, we still are not able to significantly increase the accuracy of prediction of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#9 classes classification\n",
    "print('\\n\\nRESULTS WITH NO FEATURES ADDED\\n')\n",
    "cross_validation(X_no_ft_9, Y_no_ft_9, cv_param=20)\n",
    "\n",
    "\n",
    "print('\\n9 CLASSES CLASSIFICATION\\n')\n",
    "cross_validation(X_9, Y_9, cv_param=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again here, the results follow exactly the same trend than for the 5 classes case. This does not seem that the features we added have the desired effect. Let us just see whether the features importances changed from the features we added.\n",
    "\n",
    "Let us now visualise the result we'd get when we would not have added the supplementary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_2 = plot_feature_importances(X_2,Y_2,columns_2, max_depth = 7)\n",
    "features_5 = plot_feature_importances(X_5, Y_5, columns_5, max_depth = 7)\n",
    "features_9 = plot_feature_importances(X_9, Y_9,columns_9, max_depth = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not a very clear trend that comes out of the graph, namely that some features are a bit more significant than others, even if their variances are very large. Note that as they are more features, their relative magnitude of importance decreases. We can rank them in the following temptative tiers :\n",
    "\n",
    "\n",
    "- **Most useful ** : meanExp_yellowCards, games, gravity, meanExp_gravity, goals, defeats, meanIAT_gravity, weight, ties, meanIAT_redCards, meanIAT_yellowCards, height \n",
    "- **Somehow useful :** yellowCards,meanExp_yellowReds, meanIAT_yellowReds, victories\n",
    "- **Not useful : ** german, french, redCards, yellowReds, english, spanish \n",
    "\n",
    "Even though none of the features stand out as particularly important, they can be ranked quite consistently. It is interesting to see that the *gravity* feature that we added, along with the *yellowCards* weighted with the *mean_IAT* of the referee are quite useful as features, showing us that skin colour weighted by the bias of the referee helps a *little bit* to get better results, but not in a significantly better manner. The features that were not useful in the previous get still remained not so useful, and the *yellowReds* and *redCards*, even weighted with the *mean_IAT*, still remained useless. It is probably because those events are too rare to really be a consistent behaviour of a referee with respect to a particular skin coloured player. Also, note that the variances are really huge, and the ranking can change a lot from a simulation to another, as a the trees are randomly generated. An interesting factor is also that for binary classification, the *french* attribute seems very meaningful for the classification (feature importance 10%), while for the others it remains quite meaningless (4% and 2%). We choose not to keep it in our attributes later on.\n",
    "\n",
    "## 1.5. Trying to improving our model from the results\n",
    "The objective now is to simply remove the less meaningful results of our model and see whether the prediction accuracy gets improved from it. The function `prepare_features_multi_remove` keeps only some of the most relevant features, and shows them in the *features* list. Note that the features we have below are a subset of useful features, but might not exactly coincide witht the features ranked above. The idea here is mostly to illustrate the effect of removing some features from the model (*Spoiler alert :* this does not work very well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def prepare_features_multi_remove(file_,split):\n",
    "    features = ['victories','meanIAT_gravity','meanIAT_yellowCards','weight','goals','ties','games','height','meanExp_yellowCards','meanExp_gravity']\n",
    "    X,Y,columns = prepare_features_multi(file_,split)\n",
    "    X = X[features]\n",
    "    \n",
    "    return X,Y,features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now quickly go through the 2 and 9 classes cases, in order to spot whether something changed from the removal of the features. Note that we expect the results not to change very much, as the tree was not able to classify correctly the data from this features before, and there should be no reason for it to perform significantly better now that we removed the less meaningful feautres. We would expect at best the results to be obtained slightly faster, as the tree that has to be computed is slightly smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2 classes classification\n",
    "print('\\n2 CLASSES CLASSIFICATION\\n')\n",
    "X_2_few, Y_2_few, columns_2_few = prepare_features_multi_remove('Data/players_with features_no_nan_only_num_values.csv',2)\n",
    "cross_validation(X_2_few, Y_2_few, cv_param=20)\n",
    "\n",
    "print('')\n",
    "features_2 = plot_feature_importances(X_2_few,Y_2_few,columns_2_few)\n",
    "\n",
    "#9 classes classification\n",
    "print('\\n9 CLASSES CLASSIFICATION\\n')\n",
    "X_9_few, Y_9_few, columns_9_few = prepare_features_multi_remove('Data/players_with features_no_nan_only_num_values.csv',9)\n",
    "cross_validation(X_9_few, Y_9_few, cv_param=20)\n",
    "\n",
    "print('')\n",
    "features_2_few = plot_feature_importances(X_9_few,Y_9_few,columns_9_few)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no surprises in these results, which behaved as we predicted. The overall accuracy even went down a bit, as some a the features we removed might have provided the slightiest of helps in order to obtain the classification we had before. We will not comment a lot on those results, as they exhibit the same behaviour as the previous ones. Just note that the significance of every features increased, but it is only due to the fact that we are only considering 10 features now on. Every feature appears to be as ineffective as another to obtain the results we have, with some of the features we added containing the referee bias towards a player yielding a small improvement in prediction, but nothing very meaningful still. Note however that there seems to be less variance in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Conclusion\n",
    "The experiment done in determining the skin colour of the player from the dataset we were given seems to be quite unsuccessful. We were not able to either create or find in our data a feature that would systematically discrimate the players between the colour of their skin. This is reassuring in the fact that the skin colour of a player does not seems to differentiate the performance, as well as the behaviour of the referee does not seem to change significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Unsupervised Learning\n",
    "\n",
    "> Aggregate the referee information grouping by soccer player, and use an unsupervised learning technique to cluster the soccer players in 2 disjoint clusters. Remove features iteratively, and at each step perform again the clustering and compute the silhouette score -- can you find a configuration of features with high silhouette score where players with dark and light skin colors belong to different clusters? Discuss the obtained results.\n",
    "\n",
    "We now try another of performing this classification by using *unsupervised* Machine Learning. In this problem, we do not have any label for the training corresponding to the skin colour, and we try to seperate the data into two distinct clusters. However this separation is completely *unsupervised* and we have **no guanrantee that the split will be done between black and white people**, because we do not tell the algorithm on which criteria the split between the clusters should be done.\n",
    "\n",
    "The features we use are the same as previously, where we only keep the best 10 features, to help the algorithm perform the clustering on the most meaningful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "data = X_2_few\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, init = 'k-means++', random_state=0).fit(data)\n",
    "\n",
    "predictions = kmeans.labels_\n",
    "\n",
    "print(\"Silhouette score : {s}\".format(s=silhouette_score(data, kmeans.labels_)))\n",
    "\n",
    "# people belonging to cluster 0\n",
    "clust0 = Y_2_few[kmeans.labels_ == 0]\n",
    "print(\"Percentage of black people in cluster 0 : {s}\".format(s=clust0.mean()))\n",
    "\n",
    "# people belonging to cluster 1\n",
    "clust1 = Y_2_few[kmeans.labels_ == 1] \n",
    "print(\"Percentage of black people in cluster 1 : {s}\".format(s=clust1.mean()))\n",
    "\n",
    "# We choose the cluster assignement (either to Black or White) in order to maximize the correctness\n",
    "correctness = np.mean([int(np.abs(diff) < 0.5) for diff in kmeans.labels_ - Y_2])\n",
    "if correctness > 0.5:\n",
    "    labels = kmeans.labels_\n",
    "else:\n",
    "    labels = 1 - kmeans.labels_\n",
    "\n",
    "# we want the corectness to be either close to 1 (clust 1 : skin_colour 1, clust 0 : skin_colour 0)\n",
    "# or close to 0 (opposite distribution)\n",
    "correctness = np.mean([int(np.abs(diff) < 0.5) for diff in labels - Y_2])\n",
    "print(\"Correctness : {c}\".format(c=correctness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed the clustering with the *KMeans algorithm* using the *speedup method KMeans++ for initialisation*. We used the ten most relevant features that we identified in the previous part. The metric we use in order to describe whether the two clusters are well separated is the *silhouette score*, a scalar ranging between -1 and 1, which describes how similiar an object is to its own cluster compared to the other clusters. A result 1 would indicate a perfect match of the object to its cluster. Here, the resulting *silhouette score is 0.56*, which not so bad. Thus the two clusters are pretty well separated from one another. \n",
    "\n",
    "We then computed the correctness of the clustering, ie the percentage of people that are in the right cluster, considering the best labelling for the two clusters (either cluster 1 -> black and 0 -> white or the other way around). We get that *nearly 60% of people are in the correct cluster*. However given that the two populations are quite unbalanced, we should look at the percentage of black people in the two clusters and see if wee observe a significant difference. We observe that the clusters contain respectively 23.7% and 22.2% of black people. Thus the two obtained population do not make any difference between white and black people. We get the same kind of problems than for the supervised learning.\n",
    "\n",
    "We now try to remove some more features iteratively in order to see whether one can obtain better results. The results will display the evolution of the *correctness* and the *silhouette score* as we remove iteratively features from our clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correctnessTab = []\n",
    "silhouetteTab = []\n",
    "skinDifference = []\n",
    "\n",
    "# Iterating over all different features.\n",
    "for nbFeatures in range(X_2_few.shape[1]):\n",
    "    #Keeping only a certain number of features\n",
    "    X = X_2_few.iloc[:,0:nbFeatures+1]\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=2, init = 'k-means++', random_state=0).fit(X)\n",
    "    silhouetteTab.append(silhouette_score(X, kmeans.labels_))\n",
    "    \n",
    "    correctness = np.mean([int(np.abs(diff) < 0.5) for diff in kmeans.labels_ - Y_2])\n",
    "    if correctness > 0.5:\n",
    "        labels = kmeans.labels_\n",
    "    else:\n",
    "        labels = 1 - kmeans.labels_\n",
    "    \n",
    "    clust0 = Y_2_few[labels == 0]\n",
    "    \n",
    "    clust1 = Y_2_few[labels == 1]\n",
    "    \n",
    "    skinDifference.append(np.abs(clust0.mean() - clust1.mean()))\n",
    "    \n",
    "    correctness = np.mean([int(np.abs(diff) < 0.5) for diff in labels - Y_2])\n",
    "    correctnessTab.append(correctness)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(list(range(1, 11)), correctnessTab, label=\"Correctness\")\n",
    "plt.hold(True)\n",
    "plt.plot(list(range(1, 11)), silhouetteTab, 'r', label=\"Silhouette score\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('Description of the classificaion scoring and the silhouette\\n score with the number of features considered')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(list(range(1, 11)), skinDifference)\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Skin colour difference\")\n",
    "plt.title('Difference of the percentage of the white people classified\\n in the cluster 0 and the black people classified in the cluster 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that having only a few features increases the classification accuracy (i.e. the correctness). However, it **still remains very bad in terms of skin colour classification** : the difference of black people ratio between the two populations is always less than 2 points (i.e. they are spread between the two clusters). Thus we cannot find some particular features that separate white and black people into two populations. Also note that the sihouette score is not so bad, which means that the clusters are quite well separated. We see that this does not mean that the clustering separates well black and white people. \n",
    "\n",
    "Hence, we see that using unsupervised learning yielded a good silhouette score, meaning that the features are well separated in the *feature space*, but this separation is not done on the skin colour of the player, as we haven't previously found any feature that would be significantly different for players of different skin colour."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
